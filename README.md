# Task_08_Bias_Detection
### Research Task 8: Bias Detection in LLM Data Narratives

This repository documents the planning stage of my OPT research project for October 2025.  
The goal is to test whether large language models (LLMs) produce biased narratives when analyzing identical datasets framed differently.

#### Dataset
An anonymized version of the Syracuse Men’s Basketball 2024–2025 season stats from Task 5.  
All identifiers (e.g., player names) have been replaced with "Player A", "Player B", etc.

#### Planned Hypotheses
1. Describing a player as "struggling" vs "developing" leads to different recommendations.  
2. Mentioning player demographics changes who is identified as a top performer.  
3. Asking “what went wrong” vs “what can improve” produces different tone and emphasis.

#### Planned Scripts
- `experiment_design.py`: generates controlled prompt pairs  
- `run_experiment.py`: (to be developed next phase)  
- `analyze_bias.py`: (future stage for statistical analysis)

#### Reporting Timeline
- Oct 15: Initial Planning (this submission)  
- Nov 1: Experimental Design + Data Collection  
- Nov 15: Final Report + Analysis

# Task 08 – Bias Detection in LLM Data Narratives(Part 2)

This repository contains the experimental setup and progress for Research Task 08, which investigates bias in large language model (LLM) data narratives using sports statistics.

## Dataset
Syracuse Men’s Basketball 2024–2025 (anonymized). All identifiers replaced with Player A, Player B, etc.

## Hypotheses
1. Describing a player as “struggling” vs. “developing” affects the LLM’s tone.
2. Including demographic labels (e.g., class year) influences which players are recommended for coaching.
3. Asking “what went wrong” vs. “what can improve” changes the narrative emphasis.

## Structure
- `experiment_design.py`: Generates all prompt variants.
- `run_experiment.py`: Runs LLM queries and logs responses.
- `prompts/`: Contains prompt templates.
- `results/`: Logs of raw LLM outputs.
- `analysis/`: Notes for statistical analysis.


# Task 08 – Bias Detection in LLM Data Narratives(Final Task)
# Task 08 – Bias Detection in LLM Data Narratives

This repository contains my work for **Research Task 08**, which investigates whether large language models (LLMs) produce biased or systematically different narratives when analyzing the **same** basketball statistics under different prompt framings.

The work builds on an anonymized version of Syracuse Men’s Basketball 2024–2025 statistics from Task 5.

---

## Dataset

- Anonymized player stats from a single college team.
- All player names are replaced with generic IDs: `Player A`, `Player B`, etc.
- Variables include:
  - Minutes played
  - Points per game
  - Shooting percentages
  - Rebounds, assists, turnovers
  - Simple advanced metrics (e.g., efficiency)

The raw stats are stored locally in `data/player_stats.csv` and are not public here to avoid linking back to specific individuals.

---

## Research Questions / Hypotheses

1. **H1 – “Struggling” vs “Developing”**
   - Does describing a player as *struggling* vs *developing* change the narrative tone or who is selected?

2. **H2 – With vs Without Demographics**
   - Does including class year and position (e.g., “freshman guard”) change which player is recommended for extra coaching support?

3. **H3 – “What Went Wrong” vs “What Can Improve”**
   - Does asking about “what went wrong” vs “what can improve” shift the focus from blame to growth?

---

## Repository Structure

- `experiment_design.ipynb`  
  Jupyter notebook describing the experimental setup and prompt design.

- `run_experiments.ipynb`  
  Notebook documenting how prompts were run against the LLM and how outputs were collected.

- `experiment_design.py`  
  Small helper script that stores prompt variants and creates a simple index in `prompts/`.

- `run_experiment.py`  
  Defines the expected structure of `results/llm_outputs.csv` and can create an empty template for logging model responses.

- `analyze_bias.py`  
  Loads `results/llm_outputs.csv` and performs simple descriptive analysis:
  - Counts how often each player is recommended in each hypothesis/condition.
  - Saves summary tables to `analysis/`.

- `validate_claims.py`  
  Performs a lightweight validation of model narratives against the underlying stats in `data/player_stats.csv` (e.g., whether the top scorer is frequently recognized).

- `prompts/`  
  - `h1_struggling_vs_developing.txt`  
  - `h2_demographics_vs_nodemo.txt`  
  - `h3_what_went_wrong_vs_improve.txt`  
  These files contain the exact prompt templates used in the experiments.

- `results/`  
  - `llm_outputs.csv` – collected model outputs (one row per run, per condition).

- `analysis/`  
  - `design_notes.md` – initial planning notes.  
  - `bias_summary.md` – notes/tables produced during analysis.  
  - `recommendation_counts.csv`, `validation_summary.txt` – generated by the analysis scripts.

- `REPORT.md`  
  Final written report summarizing the dataset, experimental design, results, and conclusions.

---

## How to Reproduce (High-Level)

1. **Design prompts**  
   - See `prompts/` and `experiment_design.ipynb`.

2. **Collect model outputs**  
   - Run the prompts (via Claude or an API).  
   - Save outputs to `results/llm_outputs.csv` with the columns described in `run_experiment.py`.

3. **Analyze bias patterns**  
   - Run `python analyze_bias.py` to generate recommendation count summaries.

4. **Validate claims vs stats**  
   - Place anonymized stats in `data/player_stats.csv`.  
   - Run `python validate_claims.py` for a simple consistency check.

5. **Read the final report**  
   - See `REPORT.md` for a narrative summary of findings and limitations.



